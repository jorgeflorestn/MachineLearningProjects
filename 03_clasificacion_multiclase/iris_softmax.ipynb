{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación multiclase softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clasificación softmax es una generalización de la regresión logística binaria, donde en vez de dividir o separar mediante un valor delimitante se asignan valores de probabilidad mediante la 'normalización' de valores de salida o las clases mediante la funcion *softmax* o también conocido como *función exponencial normalizada*\n",
    "\n",
    "Esta regresión es un es un algoritmo destinado clasificar los datos para más de 2 clases. En realidad, es una generalización de lo que se usa en la regresión logística binaria, como se verá a continuación\n",
    "\n",
    "La gran diferencia entre la regresión binaria es que es necesario una cierta normalización los datos tanto de entrada como de salida, por lo que se requiere 2 pasos para poder llevar a cabo una transformación de los datos de tal forma que se cumpla con el ajuste analizado anteriormente del descenso del gradiente.\n",
    "\n",
    "Al normalizar los valores de entrada y de salida, se obtienen probabilidades de los cuales al sumarlas se obtiene un total de uno, es decir, es una distribución de probabilidades.\n",
    "\n",
    "No hay mucha diferencia en la parte práctica comparado con la regresión logística binaria, pues hay que tener cuidado de cómo presentar al algoritmo la salida o las clases de las cuales se van a clasificar. Tal como se explica en las imágenes siguientes, el cual es simplemente intercambiar la función sigmoide por la función softmax, que, por fin es prácticos, en esa libreta, se llamarán de esta manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import Bunch\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "#  logging.disable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desarrollo de teoria"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La imagen anterior se puede observar un comportamiento esperado de una función matricial de datos, el cual las operaciones aplicadas sobre los pesos o características esenciales de la base de datos forman parte de la toma de decisiones en la clasificación. Se agrega entonces la operación sigmoide o softmax para normalizar y preparar la regresión aplicable en probabilidades de cero a uno, lo que nosotros conocemos como los o la matriz theta.\n",
    "\n",
    "Al decir que las operaciones son similares quiere decir que la transformación matricial compone de una misma sintaxis comparado con la regresión logística binaria, es preparar la entrada y la salida de datos.\n",
    "\n",
    "El único inconveniente encontrado es la necesidad de transformar la salida de una forma nominal, es decir, responder a la siguiente pregunta: ¿es o no la clase uno, o la clase dos, etcétera?\n",
    "\n",
    "Deben cumplir con la siguientes condiciones.\n",
    "\n",
    "Desarrollo de la función softmax como:\n",
    "\n",
    "$$\n",
    "\n",
    "P(y=j \\mid z^{(i)}) = \\phi(z^{(i)}) = \\frac{e^{z^{(i)}}}{\\sum_{j=1}^{k} e^{z_{j}^{(i)}}}\n",
    "\n",
    "$$\n",
    "\n",
    "Donde los dados Z como:\n",
    "\n",
    "$$\n",
    "z = w_1x_1 + ... + w_mx_m  + b= \\sum_{l=1}^{m} w_l x_l + b= \\mathbf{w}^T\\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "O de forma simplficada\n",
    "\n",
    "$$\n",
    "\\mathbf{Z} = \\mathbf{X}\\mathbf{W} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "En muchas ocasiones se omitirá la b. \n",
    "\n",
    "Este algoritmo es directamente aplicable con el descenso de gradiente."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ventajas y desventajas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ventajas:\n",
    "- Este algoritmo al ser extensión la regresión logística binaria la programación se modifica únicamente sustituyendo la función sigmoide por softmax.\n",
    "\n",
    "- Requiere pocos pasos comparado con las versiones anteriores de clasificación one vs all y one vs one.\n",
    "\n",
    "- Al igual, su preparación de datos es igual de reducida.\n",
    "\n",
    "Desventajas\n",
    "- La aplicación del descenso del gradiente resulta afectarle demasiado dando como resultado una baja eficiencia de datos finales de predicción.\n",
    "\n",
    "- Es muy sencillo en el sentido de que se observan pocas características en la base de datos reducir a los posibles resultados con una precisión mayor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de preparacion de datos y entrenamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es algoritmo transforma los datos de salida de una forma nominal, es decir, en extender las clases el número de columnas y decir si la columna uno verdadero en caso de pertenecer a la clase uno y dejar como cero el resto de clases posibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(target: np.ndarray) -> np.ndarray:\n",
    "    n_classes: int = np.unique(target).shape[0]\n",
    "    y_encode: np.ndarray = np.zeros((target.shape[0], n_classes))\n",
    "    for idx, val in enumerate(target):\n",
    "        y_encode[idx, val] = 1.0\n",
    "    return y_encode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z: np.ndarray) -> np.ndarray:\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(\n",
    "    data: np.ndarray, target: np.ndarray, eta: float = 0.55, iterations: int = 100000\n",
    ") -> np.ndarray:\n",
    "    m = len(target)\n",
    "    logging.info(f\"target: {m}\")\n",
    "\n",
    "    theta = np.random.randn(data.shape[1], target.shape[1])\n",
    "\n",
    "    logging.info(f\"theta: {theta}\")\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        gradients = (1 / m) * (data.T @ (softmax(data @ theta) - target))\n",
    "        theta = theta - eta * gradients\n",
    "\n",
    "    logging.info(f\"theta: {theta}\")\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones y métodos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(\n",
    "    sepal_length: float,\n",
    "    sepal_width: float,\n",
    "    petal_length: float,\n",
    "    petal_width: float,\n",
    "    weights: np.ndarray,\n",
    ") -> list[int]:\n",
    "    list1 = [0, 0, 0]\n",
    "\n",
    "    for i in range(len(list1)):\n",
    "        a0 = weights.T[i][0]\n",
    "        a1 = weights.T[i][1]\n",
    "        a2 = weights.T[i][2]\n",
    "        a3 = weights.T[i][3]\n",
    "        a4 = weights.T[i][4]\n",
    "        list1[i] = np.exp(\n",
    "            a0 + a1 * sepal_length + a2 * sepal_width + a3 * petal_length + a4 * petal_width\n",
    "        )\n",
    "\n",
    "    maxP = np.argmax([z / sum(list1) for z in list1])\n",
    "\n",
    "    pred = [0, 0, 0]\n",
    "\n",
    "    pred[maxP] = 1\n",
    "\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(\n",
    "    data: np.ndarray, target: np.ndarray, weights: np.ndarray\n",
    ") -> tuple[np.ndarray, float]:\n",
    "    predict_list = []\n",
    "    test_list = []\n",
    "    for i in data:\n",
    "        predict_list.append(np.argmax(model_test(i[0], i[1], i[2], i[3], weights)))\n",
    "    for j in target:\n",
    "        test_list.append(np.argmax(j))\n",
    "    num = 0\n",
    "    for k in range(len(predict_list)):\n",
    "        if predict_list[k] == test_list[k]:\n",
    "            num = num + 1\n",
    "\n",
    "    final_list: np.ndarray = np.array([predict_list, test_list], ndmin=2)\n",
    "    effi = num / len(predict_list)\n",
    "\n",
    "    return final_list, effi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de datos: Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris: Bunch = datasets.load_iris()  # type: ignore\n",
    "\n",
    "x_data: np.ndarray = iris[\"data\"]  # type: ignore\n",
    "y_data: np.ndarray = iris[\"target\"]  # type: ignore\n",
    "\n",
    "x_c: np.ndarray = np.c_[np.ones((len(x_data), 1)), x_data]\n",
    "y_c = one_hot_encoder(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de datos, división en split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:x_train shape: (52, 5)\n",
      "INFO:root:x_test shape: (98, 5)\n",
      "INFO:root:y_train shape: (52, 3)\n",
      "INFO:root:y_test shape: (98, 3)\n"
     ]
    }
   ],
   "source": [
    "train_test_data: list[np.ndarray] = train_test_split(x_c, y_c, train_size=0.35)  # type: ignore\n",
    "x_train, x_test, y_train, y_test = train_test_data\n",
    "\n",
    "logging.info(f\"x_train shape: {x_train.shape}\")\n",
    "logging.info(f\"x_test shape: {x_test.shape}\")\n",
    "logging.info(f\"y_train shape: {y_train.shape}\")\n",
    "logging.info(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:target: 52\n",
      "INFO:root:theta: [[-0.52677757  1.68111362 -1.26168214]\n",
      " [ 1.39024864  0.29105891  0.27535409]\n",
      " [-1.07505919 -1.48390578 -0.38278114]\n",
      " [-0.11678292 -0.06900336  0.86388471]\n",
      " [-2.76731299 -0.19265895  1.24500431]]\n",
      "INFO:root:theta: [[  2.57933084  12.76459168 -15.45126862]\n",
      " [  9.28847076   9.96200548 -17.2938146 ]\n",
      " [  7.01033844  -5.62169492  -4.33038963]\n",
      " [-15.30264055  -8.1204134   24.10115239]\n",
      " [-10.04908556  -7.69836676  16.0324847 ]]\n"
     ]
    }
   ],
   "source": [
    "a = model_fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediccón"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Predictions: [[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      "  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      "  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " [2 1 2 1 2 0 2 2 1 2 1 1 2 2 2 0 2 1 1 1 2 2 0 1 0 0 2 0 2 2 0 0 0 1 0 1\n",
      "  0 2 0 0 0 1 0 1 0 1 1 0 1 1 1 2 1 2 0 2 1 0 2 0 1 0 2 2 1 1 2 2 0 1 1 0\n",
      "  1 0 2 1 2 1 2 2 0 0 2 1 0 0 1 1 0 0 1 2 1 0 1 1 2 0]]\n",
      "INFO:root:Efficiency: 0.3163265306122449\n"
     ]
    }
   ],
   "source": [
    "predictions, efficiency = model_predict(x_test, y_test, a)\n",
    "\n",
    "logging.info(f\"Predictions: {predictions}\")\n",
    "logging.info(f\"Efficiency: {efficiency}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
